---
time: 2019.4.10 18:48:49
---
对kaggle digit_recognized项目的总结
<!--more-->
## 方案一：Random Forest
当时学的时候没认真听，趁这个机会搞一下 为我的勤奋 (゜-゜)つロ 干杯
1. 是一种在原始样本的基础上进行重采样(成为自举)，得到新的样本集，并用此样本集训练多个分类器，已多个分类器的结果进行加权得到最终结果的算法。
2. 关于Bootstrap(自举)：对给定有 N 个样本的数据集D进行Bootstrap采样,得到D_i ,在D_i上训练模型f_i 。上述过程重复M次,得到M个模型

    采样时进行有放回的抽取，一般抽取次数与原始样本数相同，得到一个样本D_i。
3. 可以证明:Bagging可以降低模型的方差,但是不能降低模型的偏差
4. 整体模型方差的降低程度与单个模型之间的相关性有关
5. 由于只是训练数据有一些不同,对决策树算法进行Bagging得到的多棵树高度相关,因此带来的方差减少有限。
6. 随机森林通过 
    1. 随机选择一部分特征 
    2. 随机选择一部分样本

    降低树的相关性。随机森林在很多应用案例上被证明有效,但牺牲了可解释性
7. #某种神秘力量导致预测阶段速度奇慢

    经调查发现是因为把矩阵分行输入了，摔，垃圾博客害人
8. 未调参情况下得分0.96371
### 方案二：Boosting
1. round1 得分0.96371 貌似过拟合了，是时候轮到我 调参侠 出场了！
2. round2 得分0.95242 看来没有过拟合，增大学习轮次试试
3. round3 得分0.96371 摔，不调了
### 方案三：MLP
1. 在240轮左右loss突然升至1.3，是它，没错！神秘力量又出现了！
2. round1 得分0.94428 瞌睡了，告辞 (๑•̀ㅂ•́)و✧

### 日记
#### day1
迷茫，不知道该干什么的时候，天上射下一道光。学习，他在向我招手。

亲爱的，我来了 ...(*￣０￣)ノ[爱你]
#### day2
跑了一晚，今早起来发现结果保存出错了。哭哭惹(ノへ￣、)

我们无法一起学习真色，不是，真甜(/▽＼)